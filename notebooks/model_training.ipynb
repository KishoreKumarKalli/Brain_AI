{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-31T06:16:47.589303Z",
     "start_time": "2025-03-31T06:16:36.961076Z"
    }
   },
   "source": [
    "# Brain AI Framework - Model Training Notebook\n",
    "# \n",
    "# This notebook demonstrates the training process for the brain segmentation and abnormality detection models using MONAI framework. It covers:\n",
    "# 1. Data loading and preparation\n",
    "# 2. Model architecture setup\n",
    "# 3. Training and validation\n",
    "# 4. Model evaluation and saving\n",
    "# \n",
    "# Author: Kishore Kumar Kalligive\n",
    "# Date: March 31, 2025\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# MONAI imports\n",
    "import monai\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, ScaleIntensityd,\n",
    "    CropForegroundd, RandCropByPosNegLabeld, RandAffined, Spacingd,\n",
    "    ToTensord, RandFlipd, RandRotate90d, RandScaleIntensityd, RandShiftIntensityd\n",
    ")\n",
    "from monai.networks.nets import UNet, DynUNet\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "from monai.losses import DiceLoss, DiceCELoss, FocalLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import list_data_collate, decollate_batch, Dataset, CacheDataset\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.data.dataset import prepare_datalist\n",
    "from src.models.segmentation import BrainSegmentationModel\n",
    "from src.models.anomaly import AnomalyDetectionModel\n",
    "from src.training.trainer import train_epoch, validate_epoch\n",
    "from src.training.loss import CombinedLoss\n",
    "from src.utils.metrics import calculate_metrics\n",
    "import yaml\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "monai.utils.set_determinism(seed=42)\n",
    "\n",
    "# Load configuration\n",
    "with open(\"../config.yml\", 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Data Loading and Preparation\n",
    "\n",
    "# Load the dataset information\n",
    "subjects_df = pd.read_csv(os.path.join(config['data']['clinical_dir'], 'ADNI_T1.csv'))\n",
    "print(f\"Total subjects: {len(subjects_df)}\")\n",
    "\n",
    "# Display the distribution of diagnostic groups\n",
    "diagnosis_count = subjects_df['Diagnosis'].value_counts()\n",
    "print(\"\\nDiagnostic group distribution:\")\n",
    "print(diagnosis_count)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "diagnosis_count.plot(kind='bar')\n",
    "plt.title('Subject Distribution by Diagnostic Group')\n",
    "plt.xlabel('Diagnostic Group')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prepare data paths and labels\n",
    "data_dir = config['data']['processed_dir']\n",
    "\n",
    "# Create data dictionaries for MONAI\n",
    "def create_data_dicts(subjects_df, data_dir):\n",
    "    train_subjects, val_subjects, test_subjects = [], [], []\n",
    "    \n",
    "    # Split data by diagnosis to ensure balanced distribution across splits\n",
    "    for diagnosis in ['CN', 'MCI', 'AD']:\n",
    "        diag_subjects = subjects_df[subjects_df['Diagnosis'] == diagnosis]\n",
    "        \n",
    "        # Get subject IDs for each diagnosis\n",
    "        subject_ids = diag_subjects['Subject_ID'].values\n",
    "        np.random.shuffle(subject_ids)\n",
    "        \n",
    "        # Split: 70% train, 15% validation, 15% test\n",
    "        n_subjects = len(subject_ids)\n",
    "        n_train = int(0.7 * n_subjects)\n",
    "        n_val = int(0.15 * n_subjects)\n",
    "        \n",
    "        train_ids = subject_ids[:n_train]\n",
    "        val_ids = subject_ids[n_train:n_train+n_val]\n",
    "        test_ids = subject_ids[n_train+n_val:]\n",
    "        \n",
    "        # Get corresponding subjects\n",
    "        train_subjects.extend(diag_subjects[diag_subjects['Subject_ID'].isin(train_ids)].to_dict('records'))\n",
    "        val_subjects.extend(diag_subjects[diag_subjects['Subject_ID'].isin(val_ids)].to_dict('records'))\n",
    "        test_subjects.extend(diag_subjects[diag_subjects['Subject_ID'].isin(test_ids)].to_dict('records'))\n",
    "    \n",
    "    # Create dictionaries with image and segmentation paths\n",
    "    train_files = [\n",
    "        {\"image\": os.path.join(data_dir, f\"{s['Subject_ID']}_t1.nii.gz\"),\n",
    "         \"label\": os.path.join(data_dir, f\"{s['Subject_ID']}_seg.nii.gz\"),\n",
    "         \"diagnosis\": s['Diagnosis']} \n",
    "        for s in train_subjects\n",
    "    ]\n",
    "    \n",
    "    val_files = [\n",
    "        {\"image\": os.path.join(data_dir, f\"{s['Subject_ID']}_t1.nii.gz\"),\n",
    "         \"label\": os.path.join(data_dir, f\"{s['Subject_ID']}_seg.nii.gz\"),\n",
    "         \"diagnosis\": s['Diagnosis']} \n",
    "        for s in val_subjects\n",
    "    ]\n",
    "    \n",
    "    test_files = [\n",
    "        {\"image\": os.path.join(data_dir, f\"{s['Subject_ID']}_t1.nii.gz\"),\n",
    "         \"label\": os.path.join(data_dir, f\"{s['Subject_ID']}_seg.nii.gz\"),\n",
    "         \"diagnosis\": s['Diagnosis']} \n",
    "        for s in test_subjects\n",
    "    ]\n",
    "    \n",
    "    return train_files, val_files, test_files\n",
    "\n",
    "# Create data splits\n",
    "train_files, val_files, test_files = create_data_dicts(subjects_df, data_dir)\n",
    "\n",
    "print(f\"Training samples: {len(train_files)}\")\n",
    "print(f\"Validation samples: {len(val_files)}\")\n",
    "print(f\"Testing samples: {len(test_files)}\")\n",
    "\n",
    "# Define data transformations for training\n",
    "train_transforms = Compose([\n",
    "    LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "    Spacingd(keys=[\"image\", \"label\"], pixdim=(1.0, 1.0, 1.0), mode=(\"bilinear\", \"nearest\")),\n",
    "    ScaleIntensityd(keys=[\"image\"], minv=0.0, maxv=1.0),\n",
    "    CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "    RandCropByPosNegLabeld(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=(96, 96, 96),\n",
    "        pos=1,\n",
    "        neg=1,\n",
    "        num_samples=4,\n",
    "    ),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "    RandScaleIntensityd(keys=[\"image\"], factors=0.1, prob=0.5),\n",
    "    RandShiftIntensityd(keys=[\"image\"], offsets=0.1, prob=0.5),\n",
    "    ToTensord(keys=[\"image\", \"label\"]),\n",
    "])\n",
    "\n",
    "# Define data transformations for validation and testing\n",
    "val_transforms = Compose([\n",
    "    LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "    Spacingd(keys=[\"image\", \"label\"], pixdim=(1.0, 1.0, 1.0), mode=(\"bilinear\", \"nearest\")),\n",
    "    ScaleIntensityd(keys=[\"image\"], minv=0.0, maxv=1.0),\n",
    "    CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "    ToTensord(keys=[\"image\", \"label\"]),\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0, num_workers=4)\n",
    "val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=1.0, num_workers=4)\n",
    "test_ds = CacheDataset(data=test_files, transform=val_transforms, cache_rate=1.0, num_workers=4)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, num_workers=4, pin_memory=True)\n",
    "\n",
    "# 2. Model Architecture Setup\n",
    "\n",
    "# Define number of output classes\n",
    "# Class 0: Background\n",
    "# Class 1: Gray matter\n",
    "# Class 2: White matter\n",
    "# Class 3: CSF/Ventricles\n",
    "# Class 4: Hippocampus\n",
    "# Class 5: Abnormality (lesions, tumors)\n",
    "num_classes = 6\n",
    "\n",
    "# Initialize the segmentation model\n",
    "model = BrainSegmentationModel(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=num_classes,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = DiceCELoss(include_background=False, to_onehot_y=True, softmax=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['training']['epochs'])\n",
    "\n",
    "# Define metrics\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "hausdorff_metric = HausdorffDistanceMetric(include_background=False, reduction=\"mean\")\n",
    "\n",
    "# 3. Training and Validation Loop\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = config['training']['epochs']\n",
    "val_interval = config['training']['val_interval']\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "train_losses = []\n",
    "val_metrics = []\n",
    "\n",
    "print(f\"Training for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"{step}/{len(train_loader)}, train_loss: {loss.item():.4f}\", end='\\r')\n",
    "        \n",
    "    epoch_loss /= step\n",
    "    train_losses.append(epoch_loss)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch + 1} training completed. Average loss: {epoch_loss:.4f}, Time: {training_time:.2f}s\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            dice_vals = []\n",
    "            hausdorff_vals = []\n",
    "            \n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = val_data[\"image\"].to(device), val_data[\"label\"].to(device)\n",
    "                \n",
    "                # Sliding window inference for large volumes\n",
    "                roi_size = (96, 96, 96)\n",
    "                sw_batch_size = 4\n",
    "                val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
    "                \n",
    "                # Post-processing (argmax)\n",
    "                val_outputs = torch.argmax(val_outputs, dim=1, keepdim=True)\n",
    "                \n",
    "                # Compute metrics\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                hausdorff_metric(y_pred=val_outputs, y=val_labels)\n",
    "                \n",
    "            # Aggregate metrics\n",
    "            dice_result = dice_metric.aggregate().item()\n",
    "            hausdorff_result = hausdorff_metric.aggregate().item()\n",
    "            dice_metric.reset()\n",
    "            hausdorff_metric.reset()\n",
    "            \n",
    "            val_metrics.append(dice_result)\n",
    "            \n",
    "            if dice_result > best_metric:\n",
    "                best_metric = dice_result\n",
    "                best_metric_epoch = epoch + 1\n",
    "                # Save the best model\n",
    "                torch.save(model.state_dict(), os.path.join(config['training']['model_dir'], \"best_model.pth\"))\n",
    "                print(\"Saved new best model!\")\n",
    "                \n",
    "            print(\n",
    "                f\"Validation Dice score: {dice_result:.4f}, \"\n",
    "                f\"Hausdorff distance: {hausdorff_result:.4f}, \"\n",
    "                f\"Best Dice: {best_metric:.4f} at epoch {best_metric_epoch}\"\n",
    "            )\n",
    "\n",
    "# 4. Model Evaluation on Test Set\n",
    "\n",
    "print(\"\\nEvaluating best model on test dataset...\")\n",
    "model.load_state_dict(torch.load(os.path.join(config['training']['model_dir'], \"best_model.pth\")))\n",
    "model.eval()\n",
    "\n",
    "test_dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=True)\n",
    "test_hausdorff_metric = HausdorffDistanceMetric(include_background=False, reduction=\"mean\", get_not_nans=True)\n",
    "\n",
    "# Test time augmentation transforms\n",
    "test_time_aug_transforms = [\n",
    "    # Original\n",
    "    lambda x: x,\n",
    "    # Flips\n",
    "    lambda x: torch.flip(x, dims=[-1]),\n",
    "    lambda x: torch.flip(x, dims=[-2]),\n",
    "    lambda x: torch.flip(x, dims=[-3]),\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        test_inputs, test_labels = test_data[\"image\"].to(device), test_data[\"label\"].to(device)\n",
    "        \n",
    "        # Test-time augmentation (TTA)\n",
    "        test_outputs = []\n",
    "        roi_size = (96, 96, 96)\n",
    "        sw_batch_size = 4\n",
    "        \n",
    "        for aug_transform in test_time_aug_transforms:\n",
    "            # Apply augmentation\n",
    "            aug_inputs = aug_transform(test_inputs)\n",
    "            # Inference\n",
    "            aug_outputs = sliding_window_inference(aug_inputs, roi_size, sw_batch_size, model)\n",
    "            # Reverse augmentation on outputs\n",
    "            rev_aug_outputs = aug_transform(aug_outputs)\n",
    "            test_outputs.append(rev_aug_outputs)\n",
    "        \n",
    "        # Average predictions from TTA\n",
    "        test_outputs = torch.mean(torch.stack(test_outputs), dim=0)\n",
    "        \n",
    "        # Apply argmax to get the predicted labels\n",
    "        test_outputs = torch.argmax(test_outputs, dim=1, keepdim=True)\n",
    "        \n",
    "        # Compute metrics for each test case\n",
    "        test_dice_metric(y_pred=test_outputs, y=test_labels)\n",
    "        test_hausdorff_metric(y_pred=test_outputs, y=test_labels)\n",
    "        \n",
    "        # Save example segmentations\n",
    "        if np.random.rand() < 0.1:  # Save 10% of test predictions for visualization\n",
    "            subject_id = test_data['image_meta_dict']['filename_or_obj'][0].split('/')[-1].split('_')[0]\n",
    "            output_dir = os.path.join(config['visualization']['output_dir'], 'test_predictions')\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Save the central slice of each prediction\n",
    "            for i in range(1, num_classes):\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                \n",
    "                # Find central slice for visualization\n",
    "                z_dim = test_outputs.shape[-1] // 2\n",
    "                \n",
    "                # Original image\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.imshow(test_inputs[0, 0, :, :, z_dim].cpu().numpy(), cmap='gray')\n",
    "                plt.title('Original MRI')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                # Ground truth segmentation\n",
    "                plt.subplot(1, 3, 2)\n",
    "                gt_mask = (test_labels[0, 0, :, :, z_dim] == i).cpu().numpy()\n",
    "                plt.imshow(test_inputs[0, 0, :, :, z_dim].cpu().numpy(), cmap='gray')\n",
    "                plt.imshow(gt_mask, alpha=0.5, cmap='jet')\n",
    "                plt.title(f'Ground Truth Class {i}')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                # Predicted segmentation\n",
    "                plt.subplot(1, 3, 3)\n",
    "                pred_mask = (test_outputs[0, 0, :, :, z_dim] == i).cpu().numpy()\n",
    "                plt.imshow(test_inputs[0, 0, :, :, z_dim].cpu().numpy(), cmap='gray')\n",
    "                plt.imshow(pred_mask, alpha=0.5, cmap='jet')\n",
    "                plt.title(f'Prediction Class {i}')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_dir, f'{subject_id}_class{i}.png'))\n",
    "                plt.close()\n",
    "\n",
    "# Aggregate test metrics\n",
    "test_dice = test_dice_metric.aggregate().cpu().numpy()\n",
    "test_hausdorff = test_hausdorff_metric.aggregate().cpu().numpy()\n",
    "\n",
    "# Print per-class metrics\n",
    "print(\"\\nTest Metrics (per class):\")\n",
    "for i in range(num_classes-1):  # Skip background class (0)\n",
    "    print(f\"Class {i+1}: Dice = {test_dice[i]:.4f}, HD = {test_hausdorff[i]:.4f}\")\n",
    "\n",
    "# Print average metrics\n",
    "print(f\"\\nAverage Dice score: {np.mean(test_dice):.4f}\")\n",
    "print(f\"Average Hausdorff Distance: {np.mean(test_hausdorff):.4f}\")\n",
    "\n",
    "# 5. Plot Training Curves\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot validation Dice score\n",
    "val_epochs = [i*val_interval for i in range(len(val_metrics))]\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_epochs, val_metrics, label='Validation Dice')\n",
    "plt.title('Validation Dice Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Dice Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config['visualization']['output_dir'], 'training_curves.png'))\n",
    "plt.show()\n",
    "\n",
    "# 6. Save Final Model\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"train_losses\": train_losses,\n",
    "    \"val_metrics\": val_metrics,\n",
    "    \"test_dice\": test_dice,\n",
    "    \"test_hausdorff\": test_hausdorff,\n",
    "    \"best_metric\": best_metric,\n",
    "    \"best_metric_epoch\": best_metric_epoch,\n",
    "    \"config\": config,\n",
    "}, os.path.join(config['training']['model_dir'], \"final_model.pth\"))\n",
    "\n",
    "print(\"\\nTraining completed. Final model saved.\")\n",
    "\n",
    "# 7. Create Learning Curves Table\n",
    "results_df = pd.DataFrame({\n",
    "    'Epoch': list(range(1, num_epochs + 1)),\n",
    "    'Training Loss': train_losses,\n",
    "})\n",
    "\n",
    "# Add validation metrics\n",
    "val_results = np.full(num_epochs, np.nan)\n",
    "for i, epoch in enumerate(val_epochs):\n",
    "    val_results[epoch-1] = val_metrics[i]\n",
    "results_df['Validation Dice'] = val_results\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv(os.path.join(config['training']['model_dir'], 'training_results.csv'), index=False)\n",
    "print(\"Learning curves data saved to CSV.\")"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'prepare_datalist' from 'src.data.dataset' (D:\\KLU 4th YEAR\\Projects\\Brain_AI\\src\\data\\dataset.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 42\u001B[39m\n\u001B[32m     39\u001B[39m sys.path.append(\u001B[33m'\u001B[39m\u001B[33m..\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     41\u001B[39m \u001B[38;5;66;03m# Import project modules\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msrc\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdata\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdataset\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m prepare_datalist\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msrc\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodels\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msegmentation\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m BrainSegmentationModel\n\u001B[32m     44\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msrc\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodels\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01manomaly\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AnomalyDetectionModel\n",
      "\u001B[31mImportError\u001B[39m: cannot import name 'prepare_datalist' from 'src.data.dataset' (D:\\KLU 4th YEAR\\Projects\\Brain_AI\\src\\data\\dataset.py)"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "71a3d0dcd0666fb3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
